# MemoryOS

## General concepts

MemoryOS is an innovative Memory Operating System designed to address the challenges faced by Large Language Models (LLMs) regarding fixed context windows and inadequate memory management. LLMs struggle with long-term memory capabilities and personalization in extended interactions due to their fixed-length context windows, leading to issues like factual inconsistencies and reduced personalization in dialogues with significant temporal gaps. MemoryOS aims to overcome these limitations by providing comprehensive and efficient memory management for AI agents.

Inspired by memory management principles in traditional operating systems, MemoryOS introduces a hierarchical storage architecture and is composed of four core functional modules: Memory Storage, Updating, Retrieval, and Generation. This synergistic workflow ensures holistic management of long-term conversational memory, enabling contextual coherence and personalized recall in extended dialogues.

### MemoryOS Core Modules and Elixir Implementation Approach

MemoryOS is built around four primary modules that coordinate to establish a unified memory management framework:

1. **Memory Storage:** Organizes and stores information across three hierarchical units: Short-Term Memory (STM), Mid-Term Memory (MTM), and Long-Term Personal Memory (LPM).
2. **Memory Updating:** Manages the dynamic refreshing of memory, including transfers between STM to MTM and MTM to LPM, using strategies like FIFO and heat-based eviction.
3. **Memory Retrieval:** Leverages semantic segmentation and other strategies to query the memory tiers for relevant information.
4. **Response Generation:** Integrates retrieved memory information to produce coherent and personalized responses.

For an Elixir implementation, each of these modules could logically correspond to a GenServer, or a set of GenServers, providing isolated state management, concurrent processing, and fault tolerance within a supervision tree.

#### 1. Memory Storage Module

The Memory Storage module organizes information into a three-tier hierarchical structure:

**1.1 Short-Term Memory (STM)**

* **Purpose:** Stores real-time conversation data.
* **Structure:** Composed of "dialogue pages". Each `page_i` contains the user's query (`Q`), the model's response (`R`), and a timestamp (`T`), structured as `{Q_i, R_i, T_i}`.
* **Dialogue Chain:** To maintain contextual coherence in short-term continuous dialogue exchanges, a `dialogue_chain_i` is constructed for each page. This includes the `Q_i, R_i, T_i`, and `metachain_i`. The `metachain_i` is generated by an LLM in two steps:
    1. Evaluating a new page's contextual relevance to prior pages to determine chain linkage or resetting.
    2. Summarizing all chained pages into `metachain_i`.
* **Capacity:** STM stores information in a fixed-length queue. The experimental setting defines this fixed length as **7** dialogue pages.
* **Elixir Implementation:**
  * A `GenServer` could manage the STM state, holding a `List` or `Deque` (e.g., using `:queue` module) of dialogue pages.
  * Dialogue pages can be represented as Elixir maps or structs: `%{query: ..., response: ..., timestamp: ..., meta_chain: ...}`.
  * Interaction with an external LLM service would be required for `meta_chain_i` generation. This implies API calls, perhaps wrapped in a dedicated LLM client module.

**1.2 Mid-Term Memory (MTM)**

* **Purpose:** Stores recurring topic summaries.
* **Structure:** Adopts a Segmented Paging storage architecture, grouping dialogue pages with the same topic into "segments". Each segment contains multiple pages related to a unique topic.
* **Segment Definition:** A page `page_i` is merged into a `segment_i` if its `Fscore(page_i, segment_i)` exceeds a threshold (`θ`).
* **Fscore Calculation:** `Fscore = cos(e_s, e_p) + FJacard(K_s, K_p)`.
  * `e_s` and `e_p` are embedding vectors of a segment and a dialogue page, respectively. This requires an embedding model (likely via an LLM API).
  * `K_s` and `K_p` are keyword sets summarized by LLMs for the segment and page.
  * `FJacard` is the Jaccard similarity: `|K_s ∩ K_p| / |K_s ∪ K_p|`.
* **Similarity Threshold (`θ`):** The configured value for `θ` is **0.6**.
* **Content Summarization:** The content of a segment is summarized by an LLM based on its related dialogue pages.
* **Capacity:** The maximum length of segments in MTM is set to **200**.
* **Elixir Implementation:**
  * Another `GenServer` would manage the MTM. The state could be a `Map` where keys are segment IDs and values are segment structs.
  * A segment struct might contain: `%{id: ..., topic_summary: ..., pages: [...], embedding: ..., keywords: ..., heat_score: ..., last_accessed: ..., visit_count: ...}`.
  * Embeddings and keyword extraction would require LLM interaction.
  * Cosine similarity and Jaccard similarity functions would need to be implemented for `Fscore` calculation.

**1.3 Long-Term Personal Memory (LPM)**

* **Purpose:** Ensures persistent memory of important personal details and characteristics for both the user and the AI agent, providing consistency and personalization over long-term interactions.
* **Components:**
  * **User Persona:**
    * `User Profile`: Static attributes (gender, name, birth year).
    * `User Knowledge Base (User KB)`: Dynamically stores factual information extracted and incrementally updated from past interactions. It maintains a fixed-size queue of **100** entries.
    * `User Traits`: Evolving interests, habits, and preferences. These are constructed with **90 dimensions** across three categories (basic needs/personality, AI alignment dimensions, and content platform interest tags). LLMs extract and update these dimensions to autonomously evolve traits.
  * **Agent Persona:**
    * `Agent Profile`: Fixed settings like the AI agent's role or character traits.
    * `Agent Traits`: Dynamic attributes developed through interactions, potentially including new settings or interaction history (e.g., recommended items). It maintains a fixed-size queue of **100** entries.
* **Elixir Implementation:**
  * A `GenServer` for LPM, or potentially separate GenServers for User Persona and Agent Persona if the data volume/access patterns warrant it.
  * State could be structured with maps: `%{user_profile: %{static: ..., kb: ..., traits: ...}, agent_profile: %{static: ..., traits: ...}}`.
  * User KB and Agent Traits would be `Deque`s (fixed-size queues).
  * Trait extraction and updates are LLM-driven, requiring API calls.
  * Given the "long-term" nature, this module might benefit most from persistence mechanisms beyond process state, such as a database (e.g., leveraging Ecto) or durable storage, though the paper does not specify this implementation detail. For this guide, we assume the GenServer manages the live state which might eventually be persisted.

#### 2. Memory Updating Module

This module manages the dynamic refreshing and migration of memory between tiers.

**2.1 STM-MTM Update**

* **Mechanism:** When the STM queue (fixed length of 7) reaches its maximum capacity, the oldest dialogue page is transferred from STM to MTM using a First-In-First-Out (FIFO) strategy. New dialogue pages are always appended to the queue's end.
* **Elixir Implementation:**
  * The STM `GenServer` would monitor its queue size. When it exceeds the limit, it pops the oldest page and sends a message to the MTM `GenServer` to process this page for insertion into a segment.

**2.2 MTM-LPM Update**

* **Mechanism:** MTM updates involve two operations: segment deletion and segment-to-LPM updates, both governed by a `Heat` score.
* **Heat Score Calculation:** `Heat = α ⋅ N_visit + β ⋅ L_interaction + γ ⋅ R_recency`.
  * `N_visit`: Number of times the segment has been retrieved.
  * `L_interaction`: Total number of dialogue pages within the segment.
  * `R_recency`: Time decay coefficient, calculated as `exp(-∆t / µ)`.
    * `∆t`: Time elapsed since the last access, measured in seconds.
    * `µ`: A configurable time constant, set to **1e+7** (10,000,000 seconds).
  * **Coefficients:** `α`, `β`, and `γ` are equally set to **1**.
* **Segment Eviction (Deletion):** When the number of segments exceeds MTM's maximum capacity (200), segments with the lowest heat score are evicted. This retains topics with high engagement frequency.
* **Segment Transfer to LPM:** Segments with a `Heat` score exceeding a threshold `τ` are transferred to LPM. The threshold `τ` is set to **5**.
* **Elixir Implementation:**
  * The MTM `GenServer` would periodically calculate heat scores for its segments.
  * Upon receiving a new page (from STM), it would attempt to merge it into an existing segment based on `Fscore`. If no suitable segment exists, a new segment is created.
  * A background process (e.g., a `Task.Supervisor` managed task or a periodic `send_after`) could trigger segment eviction if capacity is reached.
  * When a segment's heat score crosses `τ`, the MTM `GenServer` sends a message to the LPM `GenServer` to process the segment for updating `User Traits`, `User KB`, and `Agent Traits`.
  * After a segment is transferred to LPM, its `L_interaction` value is reset to zero, causing its heat score to decline, preventing redundancy and promoting continuous persona evolution.

#### 3. Memory Retrieval Module

This module retrieves information from STM, MTM, and LPM based on a user query to return the most relevant information.

**3.1 STM Retrieval**

* **Mechanism:** All dialogue pages are retrieved from STM, as it holds the most recent contextual memory.
* **Elixir Implementation:** The `MemoryRetrieval` module (or its `GenServer`) would query the STM `GenServer` to retrieve its entire dialogue page list.

**3.2 MTM Retrieval**

* **Mechanism:** Employs a two-stage retrieval process inspired by psychological memory recall:
    1. **Segment Selection:** `top-m` candidate segments are selected from MTM based on their `Fscore` (semantic and keyword similarity, Eq. 3) with the query. `top-m` is set to **5**.
    2. **Page Selection:** Within these selected segments, the `top-k` most relevant dialogue pages are selected based on semantic similarity to the query. `top-k` is set to **5** for the GVD dataset and **10** for the LoCoMo benchmark.
* **Post-Retrieval:** After retrieval, the segment's `N_visit` (retrieval count) and `R_recency` (time decay coefficient) factors are updated to reflect its recent access.
* **Elixir Implementation:**
  * The `MemoryRetrieval` module would query the MTM `GenServer`.
  * The MTM `GenServer` would perform the two-stage lookup, calculating `Fscore` for segments and semantic similarity for pages.
  * It would also update the `N_visit` and `R_recency` for the retrieved segments.

**3.3 LPM Retrieval**

* **Mechanism:**
  * `User KB` and `Agent Traits`: Retrieve the `top-10` entries with the highest semantic relevance to the query vector as background knowledge.
  * `User Profile`, `Agent Profile`, and `User Traits`: All information from these components is utilized, as they store user preferences, agent characteristics, and user-specific traits.
* **Elixir Implementation:**
  * The `MemoryRetrieval` module would query the LPM `GenServer`.
  * The LPM `GenServer` would retrieve relevant data from `User KB` and `Agent Traits` based on semantic similarity (again, requiring embeddings) and provide all static and trait data from `User Profile`, `Agent Profile`, and `User Traits`.

#### 4. Response Generation Module

* **Mechanism:** The final prompt for the LLM is constructed by integrating the retrieved content from STM, MTM, and LPM, along with the user's original query.
* **Outcome:** This comprehensive integration ensures responses are contextually coherent with current interactions, draw on historical dialogue details and summaries for depth, and align with user and assistant identities, enabling coherent, accurate, and personalized interaction experiences.
* **Elixir Implementation:**
  * This module would orchestrate the calls to the `MemoryRetrieval` module for each memory tier.
  * It then constructs a structured prompt (e.g., a string or a list of messages for a chat-based LLM API) using the original query and the retrieved context.
  * Finally, it sends this prompt to the external LLM service to generate the final response.

### General Elixir Implementation Considerations

* **OTP (Open Telecom Platform):** Leverage Elixir's OTP principles.
  * **GenServers:** As outlined above, each core module (Storage, Updating, Retrieval, Generation orchestration) could be a `GenServer` responsible for its specific state and logic. This provides encapsulation and concurrency.
  * **Supervision Tree:** Organize these `GenServers` under a `Supervisor` to ensure automatic restarts and fault tolerance. For example, a main `MemoryOS.Supervisor` could oversee `MemoryOS.STM`, `MemoryOS.MTM`, `MemoryOS.LPM`, and `MemoryOS.Retrieval`.
  * **Registry/Name Registration:** Use `Registry` or `{:global, name}` to easily locate and communicate with the different memory `GenServers` from other parts of the application.
* **Data Structures:**
  * Use Elixir structs for defining the schema of complex data like `DialoguePage`, `Segment`, `UserPersona`, etc., enhancing readability and compile-time checks.
  * For fixed-size queues like STM, User KB, and Agent Traits, the `:queue` module in Erlang's standard library is ideal.
* **Concurrency:** Elixir's lightweight processes are perfect for handling simultaneous requests. Be mindful of potential race conditions when updating shared memory states (e.g., heat scores). `GenServers` inherently handle this by serializing requests to their state.
* **External LLM Integration:**
  * Create dedicated client modules for interacting with LLM APIs (e.g., for embeddings, summarization, generation). Use HTTP clients like `Finch` or `HTTPoison`.
  * Handle API keys, rate limits, and network errors gracefully.
  * Consider `circuit_breaker` libraries to prevent cascading failures if the LLM service becomes unavailable.
* **Semantic Similarity:**
  * Implement cosine similarity for vector embeddings. The embeddings themselves would come from an LLM API.
  * Implement Jaccard similarity for keyword sets.
* **Testing:**
  * **Unit Tests:** Thoroughly test the logic within each `GenServer` module (e.g., `Fscore` calculation, heat score updates, FIFO logic).
  * **Integration Tests:** Test the interactions between `GenServers` (e.g., STM to MTM updates) and with mock LLM services.
  * **End-to-End Tests:** Simulate full conversation flows to validate the overall system coherence and personalization.
* **Observability:** Implement logging (using `Logger`) and metrics (using `telemetry`) to monitor the health and performance of the memory system (e.g., queue sizes, heat scores, LLM call counts, retrieval latency).
* **Configuration:** Use Elixir's `config.exs` or `Runtime.Config` for managing hyperparameters like `STM` length, `MTM` max segments, `LPM` KB/Trait capacities, `Heat` coefficients (`α, β, γ`), `Heat` threshold (`τ`), `Fscore` threshold (`θ`), `top-m`, `top-k`, and time constant (`µ`).

## Ash implementation

Imagine MemoryOS as a core **Ash Domain** dedicated to intelligent memory management for your AI agents, structured with various **Ash Resources** and **Actions** to overcome the inherent limitations of fixed context windows in Large Language Models (LLMs). This domain would empower your Ash AI agents with robust long-term memory capabilities and enhanced personalization.

Here's how MemoryOS concepts could manifest within an Ash framework:

1. **The `MemoryOS` Ash Domain:** This central domain would encapsulate the entire memory management logic, serving as a dedicated service for any `Ash.AI.Agent` within your application that requires sophisticated memory.

2. **Hierarchical Memory Storage as Ash Resources:**
    * **Short-Term Memory (STM) Resource:** This would be an Ash Resource, perhaps named `DialoguePage`, designed to store real-time conversational data. Each instance of `DialoguePage` would contain the user's query, the agent's response, and a timestamp, potentially with a computed `dialogue_chain` attribute to maintain contextual coherence. An Ash action could manage a fixed-length queue for these pages.
    * **Mid-Term Memory (MTM) Resource:** An Ash Resource, like `DialogueSegment`, representing a group of `DialoguePage` records related to a unique topic. This resource would mimic the segmented paging architecture inspired by operating systems. Each `DialogueSegment` would have a summary generated by an LLM based on its contained pages. Crucially, an Ash calculation or action could define and compute the `Fscore` (based on semantic and keyword similarities) for merging `DialoguePage` instances into `DialogueSegment` instances.
    * **Long-Term Personal Memory (LPM) Resource:** This would be composed of two distinct Ash Resources: `UserPersona` and `AgentPersona`.
        * The `UserPersona` resource could have static attributes (gender, name, birth year), a `UserKnowledgeBase` (an Ash attribute or nested resource with a FIFO queue for factual information extracted from interactions), and `UserTraits` (another Ash attribute or nested resource representing evolving interests, habits, and preferences, updated in 90 dimensions across three categories).
        * The `AgentPersona` resource would similarly contain `AgentProfile` (fixed settings like role or character traits) and `AgentTraits` (dynamic attributes from interactions, also managed with a fixed-size FIFO queue).

3. **Memory Updating as Ash Actions and Policies:**
    * **STM-to-MTM Update Action:** An Ash action triggered when the `DialoguePage` queue in STM reaches its capacity. This action would implement a First-In-First-Out (FIFO) strategy to migrate the oldest `DialoguePage` to MTM, potentially initiating the creation of a new `DialogueSegment` or merging into an existing one based on `Fscore`.
    * **MTM-to-LPM Update Action:** A scheduled Ash action that evaluates the `Heat` score of `DialogueSegment` instances. This `Heat` score, defined as a calculated attribute `α·Nvisit + β·Linteraction + γ·Rrecency`, would determine which `DialogueSegment` instances are promoted to LPM (e.g., when `Heat` exceeds a threshold `τ=5`). Ash's declarative rules could manage segment eviction (deletion) when MTM capacity is exceeded, prioritizing segments with lower `Heat`.
    * **LPM Update Action:** An Ash action that, upon receiving a promoted `DialogueSegment` from MTM, extracts and updates the relevant `UserTraits`, `UserKnowledgeBase`, and `AgentTraits` within the respective `UserPersona` and `AgentPersona` resources. Ash's data transformation capabilities could facilitate the LLM-driven extraction and updating of these dimensions.

4. **Memory Retrieval as Ash Queries and Aggregations:**
    * **STM Retrieval:** An Ash query to retrieve all `DialoguePage` instances from STM, as it holds the most recent conversational context.
    * **MTM Retrieval:** A two-stage Ash query:
        * First, an Ash query to select top-`m` candidate `DialogueSegment` instances based on semantic similarity to the user's query (using the `Fscore` calculation).
        * Second, nested Ash queries within those segments to retrieve the top-`k` most relevant `DialoguePage` instances based on semantic similarity. Ash's `Ash.Resource.Api` could handle the updating of `Nvisit` and `Rrecency` attributes on the retrieved segments.
    * **LPM Retrieval:** Ash queries to retrieve the top-10 semantically relevant entries from `UserKnowledgeBase` and `AgentTraits`, along with all information from `UserProfile`, `AgentProfile`, and `UserTraits` within the `UserPersona` and `AgentPersona` resources.

5. **Response Generation as an `Ash.AI.Agent` Orchestration:**
    An `Ash.AI.Agent` would orchestrate the final response generation. It would execute the necessary Ash queries to the `MemoryOS` domain (STM, MTM, LPM retrieval actions). The retrieved memory content, along with the user's current query, would be assembled into a coherent prompt by the Ash Agent before being sent to the underlying LLM. This ensures that the generated responses are contextually coherent, draw upon historical dialogue, and align with user and agent identities, creating a more personalized interaction experience.

By structuring MemoryOS in this manner within the Ash framework, you would gain a declarative, robust, and scalable system for managing AI agent memory. The experimental results validate that this approach leads to significant improvements in conversational coherence, personalization, and efficiency, outperforming baselines like MemGPT and A-Mem on challenging benchmarks. The modular nature of Ash would also allow for easy extension and customization of each memory component.
